{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlopez1288/-Python-projects-Data-201/blob/main/Copy_of_Jennifer_Lopez_HW_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data Analytics Homework 04\n",
        "\n",
        "*Complete this assignment in Google Colab. Prior to submitting a copy of this notebook (.ipynb format), run every cell and ensure you have corrected all runtime errors. Be sure to fill in your Name and SUID in the following cell. As always, you must do your own work. This means you may not use answers to the following questions generated by any other person or a generative AI tool such as ChatGPT. You may, however, discuss this assignment with others in a general way and seek help when you need it, but, again, you must do your own work.*"
      ],
      "metadata": {
        "id": "neAV6tdFTY3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Jennifer Lopez\n",
        "\n",
        "SUID: 264179713"
      ],
      "metadata": {
        "id": "2i9feybmTkum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "XMhlq-j15puN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark -q"
      ],
      "metadata": {
        "id": "uUjg3KNnWWqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "spark = SparkSession\\\n",
        "    .builder\\\n",
        "    .appName('Homework 04')\\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "M4RbOhX6Wbaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment uses a data set containing information about data science programs at universities worldwide.\n",
        "\n",
        "The dataset contains many columns that we can use to understand how these data science programs differ from one another."
      ],
      "metadata": {
        "id": "5Ln-TfSHdiNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the data scince programs data set\n",
        "%%bash\n",
        "if [[ ! -f colleges-data-science-programs.csv ]]; then\n",
        " wget https://syr-bda.s3.us-east-2.amazonaws.com/colleges-data-science-programs.csv -q\n",
        "fi"
      ],
      "metadata": {
        "id": "ViSlWBg3JkBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1\n",
        "\n",
        "Read `colleges-data-science-programs.csv` into a Spark data frame named `raw_ds_programs_text`."
      ],
      "metadata": {
        "id": "2H5WOEsuHxND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules/libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DataSciencePrograms\").getOrCreate()\n",
        "\n",
        "# Define the CSV file path\n",
        "file_path = \"colleges-data-science-programs.csv\"\n",
        "\n",
        "# Download the CSV file if it doesn't exist?\n",
        "if not os.path.exists(file_path):\n",
        "    url = \"https://syr-bda.s3.us-east-2.amazonaws.com/colleges-data-science-programs.csv\"\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "# Read the CSV file into a Spark DataFrame\n",
        "raw_ds_programs_text = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KZTHlfxkLMwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not modify\n",
        "print('rows: ', raw_ds_programs_text.count(),\n",
        "      ', cols:', len(raw_ds_programs_text.columns))\n",
        "\n",
        "raw_ds_programs_text\\\n",
        "  .show(5)"
      ],
      "metadata": {
        "id": "3eeFC3Dmh0sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad9f367-bd0f-4cdc-8a70-04e2c06c7cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rows:  222 , cols: 28\n",
            "+---+--------------------+--------------------+--------------------+-------+-------+-----+------+--------+--------------------+-------------------+-------------------+----------------+------------+-------+----------+------------+-----+----+-------------+----------------+--------+---------+--------------------+--------+---------+---------+------+\n",
            "| id|                name|                 url|             program| degree|country|state|online|oncampus|          department|         created_at|         updated_at|university_count|program_size|courses|admit_reqs|year_founded|notes|cost|visualization|machine learning|business|databases|programminglanguages|capstone|mapreduce|part-time|ethics|\n",
            "+---+--------------------+--------------------+--------------------+-------+-------+-----+------+--------+--------------------+-------------------+-------------------+----------------+------------+-------+----------+------------+-----+----+-------------+----------------+--------+---------+--------------------+--------+---------+---------+------+\n",
            "|  1|South Dakota Stat...|http://www.sdstat...|        Data Science|Masters|     US|   SD| false|    true|Mathematics and S...|2015-01-10 04:13:13|2015-01-10 04:13:13|               1|        NULL|   NULL|      NULL|        NULL| NULL|NULL|         NULL|            NULL|    NULL|     NULL|                NULL|    NULL|     NULL|     NULL|  NULL|\n",
            "|  2|Dakota State Univ...|http://www.dsu.ed...|           Analytics|Masters|     US|   SD|  true|    true|Business and Info...|2015-01-10 04:13:13|2015-01-10 04:13:13|               1|        NULL|   NULL|      NULL|        NULL| NULL|NULL|         NULL|            NULL|    NULL|     NULL|                NULL|    NULL|     NULL|     NULL|  NULL|\n",
            "|  3|    Lewis University|http://www.lewisu...|        Data Science|Masters|     US|   IL|  true|    true|    Computer Science|2015-01-10 04:13:13|2015-01-10 04:13:13|               1|        NULL|   NULL|      NULL|        NULL| NULL|NULL|         NULL|            NULL|    NULL|     NULL|                NULL|    NULL|     NULL|     NULL|  NULL|\n",
            "|  4|Saint Joseph's Un...|http://online.sju...|Business Intellig...|Masters|     US|   PA|  true|    true|            Business|2015-01-10 04:13:13|2015-01-10 04:13:13|               1|        NULL|   NULL|      NULL|        NULL| NULL|NULL|         NULL|            NULL|    NULL|     NULL|                NULL|    NULL|     NULL|     NULL|  NULL|\n",
            "|  5| University Of Leeds|http://www.engine...|Advanced Computer...|Masters|     GB| NULL| false|    true|    Computer Science|2015-01-10 04:13:13|2015-01-10 04:13:13|               1|        NULL|   NULL|      NULL|        NULL| NULL|NULL|         NULL|            NULL|    NULL|     NULL|                NULL|    NULL|     NULL|     NULL|  NULL|\n",
            "+---+--------------------+--------------------+--------------------+-------+-------+-----+------+--------+--------------------+-------------------+-------------------+----------------+------------+-------+----------+------------+-----+----+-------------+----------------+--------+---------+--------------------+--------+---------+---------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2\n",
        "\n",
        "Starting with `raw_ds_programs_text`, create a new data frame named `ds_programs_text` which simply adds a column named `text` to the original data frame.\n",
        "\n",
        "The `text` column will be a concatenation of the following columns, separated by a space: `program`, `degree`, and `department`. You eill find the appropriate function in `pyspark.sql.functions`\n",
        "\n",
        "An example of the `ds_programs_text_df` should give you:\n",
        "\n",
        "```python\n",
        "ds_programs_text.orderBy('id').first().text\n",
        "```\n",
        "\n",
        "```console\n",
        "'Data Science Masters Mathematics and Statistics'\n",
        "```"
      ],
      "metadata": {
        "id": "OcL4Mm6xiMPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "# Create a new DataFrame with the 'text' column added\n",
        "ds_programs_text = raw_ds_programs_text.withColumn(\n",
        "    \"text\",\n",
        "    concat_ws(\" \", raw_ds_programs_text[\"program\"], raw_ds_programs_text[\"degree\"], raw_ds_programs_text[\"department\"])\n",
        ")\n"
      ],
      "metadata": {
        "id": "BJElldXki9OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not modify\n",
        "ds_programs_text.select('text')\\\n",
        "  .show(5, truncate = False)"
      ],
      "metadata": {
        "id": "lk82MX7oi_Rq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34932251-f932-4f1e-ee7a-b113191ac6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|text                                                              |\n",
            "+------------------------------------------------------------------+\n",
            "|Data Science Masters Mathematics and Statistics                   |\n",
            "|Analytics Masters Business and Information Systems                |\n",
            "|Data Science Masters Computer Science                             |\n",
            "|Business Intelligence & Analytics Masters Business                |\n",
            "|Advanced Computer Science(Data Analytics) Masters Computer Science|\n",
            "+------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3\n",
        "\n",
        "Create a pipeline named `pipe_features` that creates a new dataframe `ds_features`. The `pipe_features` pipeline should add a column, `features` to `ds_programs_text` that contains the `tfidf` of the `text` column.\n",
        "\n",
        "Make sure to create your pipeline using methodology similar to what was demonstrated in class. Aside from removing stop words and setting a minumum token length of 2, no further restrictions should be imposed on the resulting vocabulary."
      ],
      "metadata": {
        "id": "w37HWi00qKjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here that goes into steps\n",
        "\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Step 1: Tokenize the 'text' column\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words_token\")\n",
        "\n",
        "# Step 2: Remove stop words and filter tokens with length >= 2\n",
        "remover = StopWordsRemover(inputCol=\"words_token\", outputCol=\"words_cleaned\")\n",
        "# filter tokens with length >= 2 in the TF stage.\n",
        "\n",
        "# Step 3: Convert words to term frequency vectors using HashingTF\n",
        "hashing_tf = HashingTF(inputCol=\"words_cleaned\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "\n",
        "# Step 4: Compute the IDF to get TF-IDF vectors\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# Step 5: Create the pipeline\n",
        "pipe_features = Pipeline(stages=[tokenizer, remover, hashing_tf, idf])\n",
        "\n",
        "# Fit and transform the pipeline on the data\n",
        "pipe_model = pipe_features.fit(ds_programs_text)\n",
        "ds_features = pipe_model.transform(ds_programs_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-HrE5tkOqtOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not modify\n",
        "ds_features.select('features')\\\n",
        "  .show(5,\n",
        "        truncate = False)"
      ],
      "metadata": {
        "id": "4W_Uv6cvtR5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875825ba-fa4a-4ce4-810a-948d59f10b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|features                                                                                                                                                 |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(10000,[193,1695,2236,3363,8846],[3.615412302232064,0.6709733230656233,2.462732792293678,1.2962979072868075,0.2538801769623398])                         |\n",
            "|(10000,[3148,5374,8190,8846,8889],[0.8222042927895469,1.0377239189930973,2.411439497906128,0.2538801769623398,3.009276498661748])                        |\n",
            "|(10000,[1695,3363,8151,8846],[0.6709733230656233,2.592595814573615,2.411439497906128,0.2538801769623398])                                                |\n",
            "|(10000,[3148,4532,5374,6328,8846],[0.8222042927895469,2.316129318101803,2.0754478379861947,3.3277302297802827,0.2538801769623398])                       |\n",
            "|(10000,[2334,3363,7058,8151,8579,8846],[4.7140245909001735,1.2962979072868075,4.7140245909001735,4.822878995812256,4.020877410340228,0.2538801769623398])|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4\n",
        "\n",
        "Create a pipeline model called `pipe_pca` that computes the first two principle components of the `features` column created by `pipe_features`, and creates a new column named `scores`.\n",
        "\n",
        "Use `pipe_pca` to create a data frame, `ds_features_1` with the columns `id`, `name`, `url`, and `scores`.\n",
        "\n",
        "Note: Prior to computing PCA scores, you will want to scale the TF-IDF outputs. Refer to lecture notes regarding the appropriate parameters to use during this step."
      ],
      "metadata": {
        "id": "LZuOSsR9t6bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler, PCA\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Step 1: Scale the TF-IDF features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
        "\n",
        "# Step 2: Apply PCA to reduce to 2 principal components\n",
        "pca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"scores\")\n",
        "\n",
        "# Step 3: Create a pipeline with scaler and PCA\n",
        "pipe_pca = Pipeline(stages=[scaler, pca])\n",
        "\n",
        "# Step 4: Fit the pipeline on ds_features and transform it\n",
        "pipe_pca_model = pipe_pca.fit(ds_features)\n",
        "ds_pca = pipe_pca_model.transform(ds_features)\n",
        "\n",
        "# Step 5: Select the required columns\n",
        "ds_features_1 = ds_pca.select(\"id\", \"name\", \"url\", \"scores\")\n",
        "\n",
        "# Display result\n",
        "ds_features_1.select(\"scores\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "L8v228fcufka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a6b7bc1-7ddf-4f82-c7ee-1a5427188cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-10-1662353338>\", line 14, in <cell line: 0>\n",
            "    pipe_pca_model = pipe_pca.fit(ds_features)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\", line 205, in fit\n",
            "    return self._fit(dataset)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
            "    model = stage.fit(dataset)\n",
            "            ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\", line 205, in fit\n",
            "    return self._fit(dataset)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
            "    java_model = self._fit_java(dataset)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
            "    return self._java_obj.fit(dataset._jdf)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'Py4JJavaError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ConnectionRefusedError",
          "evalue": "[Errno 111] Connection refused",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1662353338>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Step 4: Fit the pipeline on ds_features and transform it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpipe_pca_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mds_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_pca_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2104\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2105\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_pdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m                         \u001b[0;31m# drop into debugger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36m_showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;34m'traceback'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;34m'ename'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpy3compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;34m'evalue'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpy3compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     }\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipython_genutils/py3compat.py\u001b[0m in \u001b[0;36msafe_unicode\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mgateway_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_return_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# Note: technically this should return a bytestring 'str' rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do not modify\n",
        "ds_features_1\\\n",
        "  .select('scores')\\\n",
        "  .show(5,\n",
        "        truncate = False)"
      ],
      "metadata": {
        "id": "oeY2mS27uh_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5\n",
        "\n",
        "In this question you will write code that makes recommendations for programs closest to a program of interest.  \n",
        "\n",
        "Create a function named `get_nearest_programs` that returns the 3 closest programs to a program of interest.\n",
        "\n",
        "The `get_nearest_programs` function should take 1 argument: `program_of_interest`. Write the function so that it returns the 3 programs (as defined by the `name` column) closest to the program argument as defined by Euclidian (L2) distance. Do not return the program of interest as one of the names.\n",
        "\n",
        "Your function should **not** consider **Bachelors** programs."
      ],
      "metadata": {
        "id": "CnmopTzCxZyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.types import DoubleType\n",
        "import math\n",
        "\n",
        "# Step 1: UDF to compute Euclidean (L2) distance between two vectors\n",
        "def euclidean_distance(v1, v2):\n",
        "    return float(math.sqrt(sum([(a - b) ** 2 for a, b in zip(v1, v2)])))\n",
        "\n",
        "distance_udf = udf(lambda x, y: euclidean_distance(x, y), DoubleType())\n",
        "\n",
        "# Step 2: Define the function\n",
        "def get_nearest_programs(program_of_interest):\n",
        "    # Filter to exclude Bachelors programs\n",
        "    grad_programs = ds_features_1.filter(~col(\"name\").contains(\"Bachelor\"))\n",
        "\n",
        "    # Get the vector for the program of interest\n",
        "    poi_vector = grad_programs.filter(col(\"name\") == program_of_interest).select(\"scores\").first()\n",
        "\n",
        "    if not poi_vector:\n",
        "        print(f\"Program '{program_of_interest}' not found.\")\n",
        "        return []\n",
        "\n",
        "    poi_vec = poi_vector[\"scores\"]\n",
        "\n",
        "    # Step 3: Compute distance for all other programs\n",
        "    with_distances = grad_programs.withColumn(\"distance\", distance_udf(col(\"scores\"), Vectors.dense(poi_vec)))\n",
        "\n",
        "    # Step 4: Exclude the program of interest and get top 3 closest programs\n",
        "    nearest_programs = with_distances\\\n",
        "        .filter(col(\"name\") != program_of_interest)\\\n",
        "        .orderBy(\"distance\")\\\n",
        "        .select(\"name\")\\\n",
        "        .limit(3)\n",
        "\n",
        "    return [row[\"name\"] for row in nearest_programs.collect()]\n",
        "\n",
        "def get_nearest_programs(program_of_interest):\n",
        "  '''\n",
        "  This function returns the 3 closest programs to a given program by calculating\n",
        "  the distance between the PCA scores of the selected program to the rest of programs\n",
        "  '''"
      ],
      "metadata": {
        "id": "Tc70OiVay1s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not modify\n",
        "get_nearest_programs('Syracuse University')"
      ],
      "metadata": {
        "id": "ZkL3UpLq0w3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6\n",
        "\n",
        "Create two Pandas dataframes `pc1` and `pc2` with the columns `word` and `absolute_loading` that contain the top 5 absolute values (descending order) of loadings."
      ],
      "metadata": {
        "id": "GhewABdh44vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Extract vocab from the TF-IDF stage in the original pipeline\n",
        "vocab = tfidf_model.vocabulary  # assuming you have access to tfidf_model from earlier step\n",
        "\n",
        "# Step 2: Get the PCA component matrix (2 x vocab_size)\n",
        "components = pca_model.pc.toArray()  # shape: (2, num_features)\n",
        "\n",
        "# Step 3: Extract the loadings for PC1 and PC2\n",
        "pc1_loadings = components[0]\n",
        "pc2_loadings = components[1]\n",
        "\n",
        "# Step 4: Create DataFrames with word and absolute loading\n",
        "pc1_df = pd.DataFrame({\n",
        "    \"word\": vocab,\n",
        "    \"absolute_loading\": np.abs(pc1_loadings)\n",
        "}).sort_values(by=\"absolute_loading\", ascending=False).head(5)\n",
        "\n",
        "pc2_df = pd.DataFrame({\n",
        "    \"word\": vocab,\n",
        "    \"absolute_loading\": np.abs(pc2_loadings)\n",
        "}).sort_values(by=\"absolute_loading\", ascending=False).head(5)\n",
        "\n",
        "# Step 5: Rename final output\n",
        "pc1 = pc1_df.reset_index(drop=True)\n",
        "pc2 = pc2_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "BE0HUTVo5PKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not modify\n",
        "display(pc1.head())\n",
        "display(pc2.head())"
      ],
      "metadata": {
        "id": "EbhBhfD253mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7\n",
        "\n",
        "Create a new pipeline called `pipe_pca_1` where you fit the maximum possible number of principal components for this dataset.\n",
        "\n",
        "Create a scree plot and a plot of cumulative variance explained (exactly 2 plots).\n",
        "\n",
        "Answer the following:\n",
        "\n",
        "1. How many principal components were able to create (the maximum number)?\n",
        "\n",
        "2. Based on either the scree or cumulative variance explained plot, how many principal components would you use if you were building a supervised machine learning model, and why?"
      ],
      "metadata": {
        "id": "rc6RBIYR6ozW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for new pipeline here\n",
        "from pyspark.ml.feature import PCA, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Get the number of features in TF-IDF\n",
        "num_features = ds_features.select(\"features\").first()[\"features\"].size\n",
        "\n",
        "# Step 2: Define new pipeline with PCA using max components\n",
        "scaler_1 = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
        "pca_full = PCA(k=num_features, inputCol=\"scaled_features\", outputCol=\"scores\")\n",
        "\n",
        "pipe_pca_1 = Pipeline(stages=[scaler_1, pca_full])\n",
        "\n",
        "# Step 3: Fit the model\n",
        "pipe_pca_1_model = pipe_pca_1.fit(ds_features)\n",
        "pca_model_full = pipe_pca_1_model.stages[-1]  # get PCA model\n",
        "\n",
        "# Step 4: Extract explained variance\n",
        "explained_variance = pca_model_full.explainedVariance.toArray()\n",
        "\n",
        "# Step 5: Plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "\n",
        "# Cumulative variance\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(np.arange(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
        "plt.title('Cumulative Variance Explained')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KGs3to-Q7TAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for scree plot here\n",
        "# Scree plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(np.arange(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance')\n"
      ],
      "metadata": {
        "id": "aUUQgBE-8taa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for cumulative variance explained plot here\n",
        "# Cumulative variance\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(np.arange(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
        "plt.title('Cumulative Variance Explained')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Cumulative Explained Variance')\n"
      ],
      "metadata": {
        "id": "lU8fVBbM8vbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*your answers here*"
      ],
      "metadata": {
        "id": "KZU1k3v27USq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8\n",
        "\n",
        "Starting with `pipe_pca_1` from the previous question, transform the pipeline and save the resulting dataframe to a variable named `pca_fun`.  \n",
        "\n",
        "Extract the output from the standard scaler column from the first row of `pca_fun` and store in a variable named `row1_centered`.\n",
        "\n",
        "Manually compute 5 PCA scores by projecting `row1_centered` onto the first 5 loading vectors which were computed in your PCA object. Save the 5 projected pca scores in a varialbe called `proj_scores`.\n",
        "\n",
        "Extract the first 5 PCA scores from the first row of the pca_fun scores column and save them in a variable named `pca_fun_scores`."
      ],
      "metadata": {
        "id": "zWp0t23g-dT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your answer here\n",
        "from pyspark.ml.feature import PCA, StandardScaler\n",
        "#Transform the data using pipe_pca_1\n",
        "pca_fun = pipe_pca_1_model.transform(ds_features)\n",
        "\n",
        "#Extract the scaled features (standar scaler output) from the first row\n",
        "row1_centered = pca_fun.select(\"scaled_features\").first()[\"scaled_features\"]\n",
        "\n",
        "#Project row1_centered manually onto the first 5 PCA loading vectors\n",
        "# Get PCA model from pipeline\n",
        "pca_model_full = pipe_pca_1_model.stages[-1]\n",
        "\n",
        "# Get first 5 principal component loading vectors\n",
        "loadings_matrix = pca_model_full.pc.toArray()\n",
        "first_5_loadings = loadings_matrix[:, :5]  # shape: (n_features, 5)\n",
        "\n",
        "# Manually project: dot product\n",
        "proj_scores = row1_centered.dot(first_5_loadings)\n",
        "\n",
        "#Extract first 5 PCA scores from the scores column in pca_fun\n",
        "pca_fun_scores = pca_fun.select(\"scores\").first()[\"scores\"][:5]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fNRR9_Hj_mN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not modify\n",
        "print(proj_scores)\n",
        "print(pca_fun_scores)"
      ],
      "metadata": {
        "id": "1m-S3iPE_qPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9\n",
        "\n",
        "Perform an **inverse transform** on the `proj_scores` variable and store the result in a variable named `inverse`.\n",
        "\n",
        "The grading cell below prints `inverse` and the original `row1_centered` data such that they are right next to each other.\n",
        "\n",
        "If `inverse` is different than `row1_centered`, explain why. How you could modify the forward and reverse transformation process such that the resulting `inverse` data almost exactly matches `row1_centered`."
      ],
      "metadata": {
        "id": "KJRPIwWaA3uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "import numpy as np\n",
        "\n",
        "# Take the first 5 loading vectors\n",
        "first_5_loadings = pca_model_full.pc.toArray()[:, :5]\n",
        "\n",
        "# Inverse transform (back to scaled feature space)\n",
        "inverse = np.dot(proj_scores, first_5_loadings.T)\n"
      ],
      "metadata": {
        "id": "MOQ39JWdBHFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*your answer here*"
      ],
      "metadata": {
        "id": "yobHBWy0BmTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not modify\n",
        "print(row1_centered[0:5])\n",
        "print(inverse[0:5])"
      ],
      "metadata": {
        "id": "zJ1LD2FqBKKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10\n",
        "\n",
        "Implement your modification so that `row1_centered` and `inverse` match almost exactly."
      ],
      "metadata": {
        "id": "1Y6gb6TUC1F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "# Get the full loading matrix (all components)\n",
        "full_loadings = pca_model_full.pc.toArray()  # shape: [num_features, num_components]\n",
        "\n",
        "# Project row1_centered onto all components\n",
        "proj_scores_full = np.dot(row1_centered, full_loadings)\n",
        "\n",
        "# Reconstruct the original scaled vector using all components\n",
        "inverse = np.dot(proj_scores_full, full_loadings.T)\n"
      ],
      "metadata": {
        "id": "QFteKT44DG_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not modify\n",
        "print(row1_centered[0:5])\n",
        "print(inverse[0:5])"
      ],
      "metadata": {
        "id": "WLQ3Yf4zDKTa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}